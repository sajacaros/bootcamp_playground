{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5ovwmgwNuM7g"
   },
   "source": [
    "# Image Classification\n",
    "\n",
    "모델이 데이터를 효율적으로 학습할 수 있도록 구현해보는게 목적입니다.\n",
    "\n",
    "* Data augmentation을 이용해 오버피팅을 방지해봅시다.\n",
    "\n",
    "기본적인 머신러닝 작업과정은 아래와 같습니다.\n",
    "\n",
    "1. Examine and understand data\n",
    "2. Build an input pipeline\n",
    "3. Build the model\n",
    "4. Train the model\n",
    "5. Test the model\n",
    "6. Improve the model and repeat the process\n",
    "\n",
    "* 모델 완성 후 평가 지표에 따라서 모델을 평가해 봅시다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gtOh3dJctz6k"
   },
   "source": [
    "## Project 설명\n",
    "### Task\n",
    "* 적은 수의 강아지와 고양이 사진을 이용해 classification을 진행해보자.\n",
    "* 주어진 데이터를 Augmentation 하는 법과 딥러닝 트레이닝 과정을 구현해 보는것이 목표입니다.\n",
    "* 데이터셋은 학습 데이터엔 강아지, 고양이 이미지 1000장이 있고, 테스트용 데이터에는 각각 500장씩 주어져있습니다.\n",
    "    * 트레이닝 시 image size 조절해 사용\n",
    "\n",
    "### Baseline\n",
    "* 기본적으로 사용하는 Convolution layers를 구성해 사용해보자.\n",
    "* 오버피팅을 방지하기 위한 다양한 방법들을 사용해보자.\n",
    "    * Data Augmentation, Dropout, Batch Normalization\n",
    "* Training\n",
    "* Evaluation\n",
    "    * 모델의 정확도와 크기를 이용해 점수를 제공하는 메트릭으로 평가해보자."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bjyhZYgLnBxL"
   },
   "source": [
    "### Import packages\n",
    "\n",
    "* 우리가 사용할 packages 를 import 하는 부분 입니다.\n",
    "* 필요에 따른 packages를 선언합니다.\n",
    "* 모델 저장을 위해서 구글 드라이브를 마운트해줍니다."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "T_K_gTJMuM7j",
    "ExecuteTime": {
     "end_time": "2023-11-28T02:53:26.995596300Z",
     "start_time": "2023-11-28T02:53:24.104663700Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "\n",
    "import os\n",
    "import requests\n",
    "import zipfile\n",
    "import time\n",
    "import copy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ],
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "jNzGW0WIKrTd",
    "ExecuteTime": {
     "end_time": "2023-11-28T02:53:27.012229Z",
     "start_time": "2023-11-28T02:53:26.998631600Z"
    }
   },
   "source": [
    "use_colab = False\n",
    "assert use_colab in [True, False]"
   ],
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mgio5jXjyVOt"
   },
   "source": [
    "### 데이터셋 다운로드\n",
    "\n",
    "* 해당 데이터는 개와 고양이 데이터로 구성되어 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "MTaZderogWxy",
    "ExecuteTime": {
     "end_time": "2023-11-28T02:53:28.301624200Z",
     "start_time": "2023-11-28T02:53:27.015221800Z"
    }
   },
   "source": [
    "if use_colab:\n",
    "    root_path = './'\n",
    "else:\n",
    "    root_path = 'data/'\n",
    "def download_and_extract(url, dest_path):\n",
    "    # 파일 이름 추출\n",
    "    filename = url.split('/')[-1]\n",
    "    file_path = os.path.join(dest_path, filename)\n",
    "\n",
    "    # 파일이 이미 존재하지 않는 경우 다운로드\n",
    "    if not os.path.exists(file_path):\n",
    "        response = requests.get(url)\n",
    "        with open(file_path, 'wb') as file:\n",
    "            file.write(response.content)\n",
    "\n",
    "    # 파일이 zip 파일인 경우 압축 해제\n",
    "    if file_path.endswith('.zip'):\n",
    "        with zipfile.ZipFile(file_path, 'r') as zip_ref:\n",
    "            zip_ref.extractall(dest_path)\n",
    "\n",
    "# 사용 예시\n",
    "_URL = 'https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip'\n",
    "download_and_extract(_URL, root_path)"
   ],
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rQ2hgNc_nfLg"
   },
   "source": [
    "### cats_and_dogs_filtered\n",
    "* |__ train\n",
    "    * |______ cats: [cat.0.jpg, cat.1.jpg, cat.2.jpg ....]\n",
    "    * |______ dogs: [dog.0.jpg, dog.1.jpg, dog.2.jpg ...]\n",
    "* |__ validation\n",
    "    * |______ cats: [cat.2000.jpg, cat.2001.jpg, cat.2002.jpg ....]\n",
    "    * |______ dogs: [dog.2000.jpg, dog.2001.jpg, dog.2002.jpg ...]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nb4kBkjd6cYE"
   },
   "source": [
    "# 데이터셋 설정\n",
    "* 다운로드 받은 데이터셋에 대한 경로를 이용하여, 데이터를 이용할 준비를 진행합니다."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "G88NI3nYuM7q",
    "ExecuteTime": {
     "end_time": "2023-11-28T02:53:28.316599200Z",
     "start_time": "2023-11-28T02:53:28.304618Z"
    }
   },
   "source": [
    "if use_colab:\n",
    "    PATH = '/content/cats_and_dogs_filtered'\n",
    "else:\n",
    "    PATH = 'data/cats_and_dogs_filtered/'\n",
    "train_dir = os.path.join(PATH, 'train')\n",
    "validation_dir = os.path.join(PATH, 'validation')"
   ],
   "execution_count": 4,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "6JDmNp3juM7u",
    "ExecuteTime": {
     "end_time": "2023-11-28T02:53:28.344088800Z",
     "start_time": "2023-11-28T02:53:28.318592400Z"
    }
   },
   "source": [
    "# directory with our training cat pictures\n",
    "train_cats_dir = os.path.join(train_dir, 'cats')\n",
    "\n",
    "# directory with our training dog pictures\n",
    "train_dogs_dir = os.path.join(train_dir, 'dogs')\n",
    "\n",
    "# directory with our validation cat pictures\n",
    "validation_cats_dir = os.path.join(validation_dir, 'cats')\n",
    "\n",
    "# directory with our validation dog pictures\n",
    "validation_dogs_dir = os.path.join(validation_dir, 'dogs')"
   ],
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dARbrBlfHnip"
   },
   "source": [
    "* 데이터셋의 구성을 살펴봅시다."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "7asbu69MuM7x",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1700574568324,
     "user_tz": -540,
     "elapsed": 7,
     "user": {
      "displayName": "Junseop So (쏘주형)",
      "userId": "07758510494740838877"
     }
    },
    "outputId": "48e652ee-5e15-4d8a-c721-7f624a4841b7",
    "ExecuteTime": {
     "end_time": "2023-11-28T02:53:28.353245500Z",
     "start_time": "2023-11-28T02:53:28.334489600Z"
    }
   },
   "source": [
    "num_cats_tr = len(os.listdir(train_cats_dir))\n",
    "print('total training cat images:', num_cats_tr)\n",
    "num_dogs_tr = len(os.listdir(train_dogs_dir))\n",
    "print('total training dog images:', num_dogs_tr)\n",
    "\n",
    "print(\"--\")\n",
    "\n",
    "num_cats_val = len(os.listdir(validation_cats_dir))\n",
    "print('total validation cat images:', num_cats_val)\n",
    "num_dogs_val = len(os.listdir(validation_dogs_dir))\n",
    "print('total validation dog images:', num_dogs_val)\n",
    "\n",
    "print(\"--\")\n",
    "\n",
    "total_train = num_cats_tr + num_dogs_tr\n",
    "print(\"Total training images:\", total_train)\n",
    "total_val = num_cats_val + num_dogs_val\n",
    "print(\"Total validation images:\", total_val)"
   ],
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total training cat images: 1000\n",
      "total training dog images: 1000\n",
      "--\n",
      "total validation cat images: 500\n",
      "total validation dog images: 500\n",
      "--\n",
      "Total training images: 2000\n",
      "Total validation images: 1000\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KFfWm7_wHq6Q"
   },
   "source": [
    "* 학습 데이터는 고양이와 강아지 이미지 각각 1000 장씩으로 구성되어있습니다.\n",
    "* 모델 평가를 위해서 각각 500장씩을 validation dataset으로 사용합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zIEiuF_u7KDn"
   },
   "source": [
    "### Data preparation 데이터 준비\n"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# 학습을 위해 데이터 증가(augmentation) 및 일반화(normalization)\n",
    "# 검증을 위한 일반화\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "    ]),\n",
    "    'validation': transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "    ]),\n",
    "}\n",
    "\n",
    "data_dir = PATH #'/content/cats_and_dogs_filtered'\n",
    "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n",
    "                                          data_transforms[x])\n",
    "                  for x in ['train', 'validation']}\n",
    "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=4,\n",
    "                                             shuffle=True, num_workers=4)\n",
    "              for x in ['train', 'validation']}\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'validation']}\n",
    "class_names = image_datasets['train'].classes\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ],
   "metadata": {
    "id": "V8ymdJWRJ1Ci",
    "ExecuteTime": {
     "end_time": "2023-11-28T02:53:28.407437100Z",
     "start_time": "2023-11-28T02:53:28.351251100Z"
    }
   },
   "execution_count": 7,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "(torch.Size([4, 3, 224, 224]), torch.Size([4]))"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, y = next(iter(dataloaders['train']))\n",
    "X.shape, y.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-28T02:53:38.862438300Z",
     "start_time": "2023-11-28T02:53:28.394616500Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UT4YTPRbzokO"
   },
   "source": [
    "### 이미지 샘플 확인"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# def imshow(inp, title=None):\n",
    "#     \"\"\"tensor를 입력받아 일반적인 이미지로 보여줍니다.\"\"\"\n",
    "#     inp = inp.numpy().transpose((1, 2, 0))\n",
    "#     inp = np.clip(inp, 0, 1)\n",
    "#     plt.imshow(inp)\n",
    "#     if title is not None:\n",
    "#         plt.title(title)\n",
    "#     plt.pause(0.001)  # 갱신이 될 때까지 잠시 기다립니다.\n",
    "# \n",
    "# \n",
    "# # 학습 데이터의 배치를 얻습니다.\n",
    "# inputs, classes = next(iter(dataloaders['train']))\n",
    "# \n",
    "# # 배치로부터 격자 형태의 이미지를 만듭니다.\n",
    "# out = torchvision.utils.make_grid(inputs)\n",
    "# \n",
    "# imshow(out, title=[class_names[x] for x in classes])"
   ],
   "metadata": {
    "id": "Ebqq8Na9Kzd-",
    "ExecuteTime": {
     "end_time": "2023-11-28T02:53:38.907498200Z",
     "start_time": "2023-11-28T02:53:38.864444300Z"
    }
   },
   "execution_count": 9,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ri70-ylUz_Bl"
   },
   "source": [
    "# 모델 구현\n",
    "\n",
    "* 직접 구현을 진행하거나 Transfer Learning을 진행할 수 있습니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "--dStHdvuM8W",
    "ExecuteTime": {
     "end_time": "2023-11-28T02:53:40.415426700Z",
     "start_time": "2023-11-28T02:53:38.887941600Z"
    }
   },
   "source": [
    "class CatAndDogClassificationModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv_layer = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3, stride=1, padding=1), # (3,224,224) => (64,224,224)\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=2, padding=1), # (3,224,224) => (64,112,112)\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1), # (64,112,112) => (128,112,112)\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, stride=2, padding=1), # (128,112,112) => (128,56,56)\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, stride=1, padding=1), # (128,56,56) => (256,56,56)\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, stride=2, padding=1), # (256,56,56) => (256,28,28)\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, stride=1, padding=1), # (256,28,28) => (256,28,28)\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, stride=2, padding=1), # (256,28,28) => (256,14,14)\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, stride=1, padding=1), # (256,14,14) => (256,14,14)\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, stride=2, padding=1), # (256,14,14) => (256,7,7)\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, stride=1, padding=1), # (256,7,7) => (256,7,7)\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, stride=2, padding=1), # (256,4,4) => (256,4,4)\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.fc_layer = nn.Sequential(\n",
    "            nn.Flatten(start_dim=1),\n",
    "            nn.Linear(in_features=256*4*4, out_features=1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=1024, out_features=256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=256, out_features=2),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.conv_layer(x)\n",
    "        x = self.fc_layer(x)\n",
    "        return x\n",
    "model = CatAndDogClassificationModel().to(device)\n",
    "model"
   ],
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "CatAndDogClassificationModel(\n  (conv_layer): Sequential(\n    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (2): ReLU()\n    (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (5): ReLU()\n    (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (8): ReLU()\n    (9): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n    (10): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (11): ReLU()\n    (12): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (13): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (14): ReLU()\n    (15): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n    (16): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (17): ReLU()\n    (18): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (19): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (20): ReLU()\n    (21): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n    (22): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (23): ReLU()\n    (24): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (25): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (26): ReLU()\n    (27): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n    (28): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (29): ReLU()\n    (30): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (31): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (32): ReLU()\n    (33): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n    (34): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (35): ReLU()\n  )\n  (fc_layer): Sequential(\n    (0): Flatten(start_dim=1, end_dim=-1)\n    (1): Linear(in_features=4096, out_features=1024, bias=True)\n    (2): ReLU()\n    (3): Linear(in_features=1024, out_features=256, bias=True)\n    (4): ReLU()\n    (5): Linear(in_features=256, out_features=2, bias=True)\n  )\n)"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 224, 224]           1,792\n",
      "       BatchNorm2d-2         [-1, 64, 224, 224]             128\n",
      "              ReLU-3         [-1, 64, 224, 224]               0\n",
      "            Conv2d-4         [-1, 64, 112, 112]          36,928\n",
      "       BatchNorm2d-5         [-1, 64, 112, 112]             128\n",
      "              ReLU-6         [-1, 64, 112, 112]               0\n",
      "            Conv2d-7        [-1, 128, 112, 112]          73,856\n",
      "       BatchNorm2d-8        [-1, 128, 112, 112]             256\n",
      "              ReLU-9        [-1, 128, 112, 112]               0\n",
      "           Conv2d-10          [-1, 128, 56, 56]         147,584\n",
      "      BatchNorm2d-11          [-1, 128, 56, 56]             256\n",
      "             ReLU-12          [-1, 128, 56, 56]               0\n",
      "           Conv2d-13          [-1, 256, 56, 56]         295,168\n",
      "      BatchNorm2d-14          [-1, 256, 56, 56]             512\n",
      "             ReLU-15          [-1, 256, 56, 56]               0\n",
      "           Conv2d-16          [-1, 256, 28, 28]         590,080\n",
      "      BatchNorm2d-17          [-1, 256, 28, 28]             512\n",
      "             ReLU-18          [-1, 256, 28, 28]               0\n",
      "           Conv2d-19          [-1, 256, 28, 28]         590,080\n",
      "      BatchNorm2d-20          [-1, 256, 28, 28]             512\n",
      "             ReLU-21          [-1, 256, 28, 28]               0\n",
      "           Conv2d-22          [-1, 256, 14, 14]         590,080\n",
      "      BatchNorm2d-23          [-1, 256, 14, 14]             512\n",
      "             ReLU-24          [-1, 256, 14, 14]               0\n",
      "           Conv2d-25          [-1, 256, 14, 14]         590,080\n",
      "      BatchNorm2d-26          [-1, 256, 14, 14]             512\n",
      "             ReLU-27          [-1, 256, 14, 14]               0\n",
      "           Conv2d-28            [-1, 256, 7, 7]         590,080\n",
      "      BatchNorm2d-29            [-1, 256, 7, 7]             512\n",
      "             ReLU-30            [-1, 256, 7, 7]               0\n",
      "           Conv2d-31            [-1, 256, 7, 7]         590,080\n",
      "      BatchNorm2d-32            [-1, 256, 7, 7]             512\n",
      "             ReLU-33            [-1, 256, 7, 7]               0\n",
      "           Conv2d-34            [-1, 256, 4, 4]         590,080\n",
      "      BatchNorm2d-35            [-1, 256, 4, 4]             512\n",
      "             ReLU-36            [-1, 256, 4, 4]               0\n",
      "          Flatten-37                 [-1, 4096]               0\n",
      "           Linear-38                 [-1, 1024]       4,195,328\n",
      "             ReLU-39                 [-1, 1024]               0\n",
      "           Linear-40                  [-1, 256]         262,400\n",
      "             ReLU-41                  [-1, 256]               0\n",
      "           Linear-42                    [-1, 2]             514\n",
      "================================================================\n",
      "Total params: 9,148,994\n",
      "Trainable params: 9,148,994\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 168.39\n",
      "Params size (MB): 34.90\n",
      "Estimated Total Size (MB): 203.87\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "summary(model, (3,224,224))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-28T02:53:42.672689500Z",
     "start_time": "2023-11-28T02:53:40.416423100Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R5eVZ_Rs0B-1"
   },
   "source": [
    "# 모델 학습 진행\n",
    "* 학습을 진행할때, train과 valid 데이터셋을 이용하여 학습과 검증을 동시에 진행합니다.\n",
    "* 각 데이터셋은 위에서 구성했기 때문에, 모델에서 사용할 데이터의 step의 길이를 batch_size를 이용해 계산해줍니다."
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(reduction='mean')\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)"
   ],
   "metadata": {
    "id": "PXdhXm541mNN",
    "ExecuteTime": {
     "end_time": "2023-11-28T02:53:42.684271100Z",
     "start_time": "2023-11-28T02:53:42.667340100Z"
    }
   },
   "execution_count": 12,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] train loss: 0.7095\n",
      "[1] valid loss: 0.6896\n",
      "[1] best model!! save best model.\n",
      "[2] train loss: 0.6964\n",
      "[2] valid loss: 0.6935\n",
      "[3] train loss: 0.6951\n",
      "[3] valid loss: 0.6935\n",
      "[4] train loss: 0.6940\n",
      "[4] valid loss: 0.6873\n",
      "[4] best model!! save best model.\n",
      "[5] train loss: 0.6937\n",
      "[5] valid loss: 0.6884\n",
      "[6] train loss: 0.7010\n",
      "[6] valid loss: 0.6893\n",
      "[7] train loss: 0.6963\n",
      "[7] valid loss: 0.6921\n",
      "[8] train loss: 0.7020\n",
      "[8] valid loss: 0.6935\n",
      "[9] train loss: 0.6959\n",
      "[9] valid loss: 0.6926\n",
      "[10] train loss: 0.6985\n",
      "[10] valid loss: 0.6937\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "min_loss = 9999\n",
    "best_model = None\n",
    "for epoch in range(10):   # 데이터셋을 수차례 반복합니다.\n",
    "\n",
    "    train_loss_list = []\n",
    "    model.train()\n",
    "    for i, (images, labels) in enumerate(dataloaders['train']):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        preds = model(images)\n",
    "        \n",
    "        loss = criterion(preds, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "     \n",
    "        train_loss_list.append(loss.item())\n",
    "    loss = np.mean(train_loss_list)\n",
    "    print(f'[{epoch + 1}] train loss: {loss:.4f}')\n",
    "    \n",
    "    valid_loss_list = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, (images, labels) in enumerate(dataloaders['validation']):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "    \n",
    "            preds = model(images)\n",
    "            loss = criterion(preds, labels)\n",
    "         \n",
    "            valid_loss_list.append(loss.item())\n",
    "    loss = np.mean(valid_loss_list)\n",
    "    print(f'[{epoch + 1}] valid loss: {loss:.4f}')\n",
    "    if loss < min_loss:\n",
    "        print(f'[{epoch +1}] best model!! save best model.')\n",
    "        min_loss = loss\n",
    "        best_model = model\n",
    "\n",
    "print('Finished Training')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-28T02:58:23.886695400Z",
     "start_time": "2023-11-28T02:53:42.685268100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0104, -0.0123],\n",
      "        [ 0.0106, -0.0125],\n",
      "        [ 0.0105, -0.0096],\n",
      "        [ 0.0104, -0.0123]]) tensor([0, 1, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "iterator = iter(dataloaders['validation'])\n",
    "next(iterator)\n",
    "next(iterator)\n",
    "images, labels = next(iterator)\n",
    "images = images.to(device)\n",
    "\n",
    "preds = best_model(images).detach().cpu()\n",
    "print(preds, labels)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-28T02:59:09.025821400Z",
     "start_time": "2023-11-28T02:59:05.165545100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-28T02:58:28.025235400Z",
     "start_time": "2023-11-28T02:58:27.983042100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-28T02:58:28.026232Z",
     "start_time": "2023-11-28T02:58:27.998238100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-28T02:58:28.037724400Z",
     "start_time": "2023-11-28T02:58:28.016216700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-28T02:58:28.049699800Z",
     "start_time": "2023-11-28T02:58:28.033737600Z"
    }
   }
  }
 ]
}
